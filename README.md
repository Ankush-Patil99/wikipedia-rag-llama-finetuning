# ğŸ“š Wikipedia RAG + LLaMA Project  
A complete end-to-end Retrieval-Augmented Generation (RAG) system built using:  
- FAISS vector search  
- MiniLM-L6 sentence embeddings  
- LLaMA language model  
- Wikipedia chunked dataset  
- Evaluation metrics + visualizations  

This project demonstrates how to build a production-style RAG pipeline from scratch â€”  
including data processing, embeddings, vector indexing, retrieval, LLM reasoning,  
evaluation, and analysis.  

---

## ğŸš€ Project Overview
This repository contains all artifacts required to fully reproduce a Wikipedia-based RAG system.  
It includes: dataset preparation, embedding generation, vector indexing with FAISS,  
retrieval logic, LLaMA integration, evaluation metrics, and exploratory visualizations.  

---

## ğŸ§© Components Included

### 1ï¸âƒ£ **Dataset Preparation**
- Process Wikipedia JSONL dumps  
- Extract abstracts + section text  
- Clean + normalize text  
- Generate overlapping chunks (400-char size, 80-char overlap)  

### 2ï¸âƒ£ **Embeddings**
- Model used: `all-MiniLM-L6-v2`  
- Batch inference for 500k+ chunks  
- Stored as `.npy` for fast loading  

### 3ï¸âƒ£ **FAISS Indexing**
- Cosine-similarity search using `IndexFlatIP`  
- L2-normalized embedding vectors  
- Stored FAISS index: `wiki.index`  

### 4ï¸âƒ£ **RAG Retrieval Pipeline**
- Embed user query  
- Perform FAISS top-k retrieval  
- Construct context-aware prompt  
- Generate response using LLaMA  

### 5ï¸âƒ£ **Evaluation & Analysis**
- ROUGE-1, ROUGE-L  
- Exact match accuracy  
- Chunk-recall inspection  
- Multi-step refinement workflow  
- Visualizations: radar charts, bar charts, heatmaps  

---
flowchart LR

  %% -------------------- DATA PIPELINE --------------------
  A[ğŸ“¥ Raw Wikipedia JSONL] --> B[ğŸ§¹ Preprocessing]
  B --> C[âœ‚ï¸ Chunking (400 chars, 80 overlap)]
  C --> D[ğŸ“„ Chunked Dataset (CSV)]

  %% -------------------- EMBEDDINGS + INDEX --------------------
  D --> E[ğŸ§  Embedding Model (MiniLM-L6-v2)]
  E --> F[ğŸ”¢ Document Embeddings (.npy)]
  F --> G[ğŸ” Build FAISS Index (IndexFlatIP)]
  G --> H[ğŸ’¾ Save Index (wiki.index)]

  %% -------------------- RETRIEVAL + PROMPTING --------------------
  I[ğŸ§‘â€ğŸ’» User Query] --> J[ğŸ” Query Embedding]
  J --> K[ğŸ” FAISS Top-K Search]
  K --> L[ğŸ“š Retrieved Chunks]
  L --> M[ğŸ“ Build RAG Prompt]

  %% -------------------- GENERATION --------------------
  M --> N[ğŸ¦™ LLaMA Response Generation]
  N --> O[ğŸ—£ï¸ Final Answer]

  %% -------------------- OPTIONAL FINE-TUNING --------------------
  F --> P[ğŸ›  Prepare SFT Dataset]
  P --> Q[ğŸ”§ LoRA / QLoRA Fine-Tuning]
  Q --> N

  %% -------------------- EVALUATION --------------------
  O --> R[ğŸ“ Evaluation (ROUGE, EM)]
  R --> S[ğŸ“Š Visualizations]

  %% -------------------- EXPORT --------------------
  S --> T[ğŸŒ Upload to HuggingFace]
  S --> U[ğŸ’» Push to GitHub]

---

## ğŸ“Š Visualizations Included

- **[Chunk Length Distribution](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/plots/chunk_length_distribution.png)**  
- **[Embedding Similarity Heatmap](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/plots/embedding_similarity_heatmap.png)**    
- **[Simulated Training Loss](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/plots/simulated_training_loss.png)**  

---

## ğŸ›  Technologies Used
- **Transformers**
- **FAISS**
- **SentenceTransformers**
- **PyTorch / CUDA**
- **Matplotlib / Seaborn**
- **Hugging Face Hub**

---
## ğŸ“¦ Repository Structure

<details>
<summary><strong>ğŸ“ data/ â€” RAG results & SFT datasets</strong></summary>

| File | Description | Link |
|------|-------------|------|
| rag_evaluation_summary.csv | Evaluation summary | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/data/rag_evaluation_summary.csv) |
| rag_final_summary.csv | Final summary metrics | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/data/rag_final_summary.csv) |
| rag_optimized_results.csv | Optimized RAG results | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/data/rag_optimized_results.csv) |
| rag_optimized_results_summary.csv | Optimized summary | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/data/rag_optimized_results_summary.csv) |
| rag_refined_results.csv | Refined evaluation | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/data/rag_refined_results.csv) |
| rag_refined_results_summary.csv | Refined evaluation summary | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/data/rag_refined_results_summary.csv) |
| rag_results_summary.csv | Consolidated summary | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/data/rag_results_summary.csv) |
| rag_test_results.csv | Test-time results | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/data/rag_test_results.csv) |
| sft_dataset.jsonl | Raw SFT dataset | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/data/sft_dataset.jsonl) |
| sft_dataset_clean.jsonl | Cleaned SFT dataset | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/data/sft_dataset_clean.jsonl) |

</details>


<details>
<summary><strong>ğŸ“ embeddings/ â€” Document embeddings</strong></summary>

| File | Description | Link |
|------|-------------|------|
| doc_embeddings.npy | Numpy document embeddings | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/embeddings/embeddings/doc_embeddings.npy) |

</details>


<details>
<summary><strong>ğŸ“ faiss_index/ â€” FAISS index & chunks</strong></summary>

| File | Description | Link |
|------|-------------|------|
| wiki.index | FAISS index file | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/faiss_index/faiss_index/wiki.index) |
| wiki_chunks.csv | Chunk metadata | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/faiss_index/faiss_index/wiki_chunks.csv) |

</details>


<details>
<summary><strong>ğŸ“ plots/ â€” Visualizations</strong></summary>

| Visualization | Link |
|--------------|------|
| Chunk Length Distribution | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/plots/chunk_length_distribution.png) |
| Embedding Similarity Heatmap | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/plots/embedding_similarity_heatmap.png) |
| Simulated Training Loss | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/plots/simulated_training_loss.png) |

</details>


<details>
<summary><strong>ğŸ“ results_table/ â€” Evaluation tables</strong></summary>

| File | Description | Link |
|------|-------------|------|
| rag_evaluation_results.csv | Evaluation results table | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/results_table/rag_evaluation_results.csv) |
| rag_final_evaluation_summary.csv | Final comparison summary | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/results_table/rag_final_evaluation_summary.csv) |

</details>


<details>
<summary><strong>ğŸ“ src/ â€” Source code</strong></summary>

| Script | Description | Link |
|--------|-------------|------|
| chunking.py | Chunk generation pipeline | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/src/chunking.py) |
| embeddings_faiss.py | Embedding + FAISS utilities | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/src/embeddings_faiss.py) |
| evaluation.py | Evaluation engine | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/src/evaluation.py) |
| preprocessing.py | Text preprocessing logic | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/src/preprocessing.py) |
| rag_engine.py | Core RAG engine implementation | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/src/rag_engine.py) |
| visualization.py | Plotting utilities | [Click Here](https://huggingface.co/ankpatil1203/Wikipedia-RAG-LLAMA-Project/blob/main/wikipedia-rag-llama-finetuning/src/visualization.py) |

</details>


---
## ğŸ“ˆ Results Summary
- Average ROUGE-1: **0.42**
- Average ROUGE-L: **0.39**
- Exact Match: **18%**
- Retrieval Recall (Top-5): **82%**
- Retrieval Recall (Top-10): **91%**

> These numbers show the system retrieves relevant Wikipedia chunks effectively,  
> and LLaMA generates context-aware summaries with strong overlap.

---

## ğŸ Conclusion
This project demonstrates a complete working implementation of a Wikipediaâ€‘scale  
RAG system with retrieval, LLM response generation, evaluations, and visualization  
suitable for GitHub or professional portfolio display.
---
## ğŸ”® Future Improvements
- Add full LLaMA fine-tuning using LoRA / QLoRA
- Replace MiniLM with modern embedding models (e5-large, SFR-Embedding)
- Add reranking (Cross-Encoder or ColBERT)
- Deploy API via FastAPI + Docker
- Add streaming UI with Gradio

---

## ğŸ‘¤ Author
**Ankush Patil**  
Machine Learning & NLP Engineer  
ğŸ“§ **Email**: ankpatil1203@gmail.com  
ğŸ’¼ **LinkedIn**: www.linkedin.com/in/ankush-patil-48989739a  
ğŸŒ **GitHub**: https://github.com/Ankush-Patil99  
